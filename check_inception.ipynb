{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    \"We need to go deeper\"\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1. Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # 2. Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # 2. Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # 2. Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d( kernel_size=3, stride=1,padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bn1 = F.relu(self.b1_1(x))\n",
    "        bn2 = F.relu(self.b2_1(F.relu(self.b2_2(x))))\n",
    "        bn3 = F.relu(self.b3_1(F.relu(self.b3_2(x))))\n",
    "        bn4 = F.relu(self.b4_1(F.relu(self.b4_2(x))))\n",
    "        return torch.cat((bn1, bn2, bn3, bn4), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "X = torch.rand(1,1,96,96)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "b1_1 = nn.LazyConv2d(96, kernel_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.5423, 0.4101, 0.0000,  ..., 0.4557, 0.4850, 0.0000],\n          [0.4090, 0.4202, 0.0393,  ..., 0.0542, 0.4327, 0.1058],\n          [0.0000, 0.5996, 0.2427,  ..., 0.4738, 0.0000, 0.4490],\n          ...,\n          [0.0000, 0.4866, 0.2158,  ..., 0.1641, 0.1960, 0.1088],\n          [0.4184, 0.3197, 0.4327,  ..., 0.0000, 0.1955, 0.3616],\n          [0.0000, 0.0460, 0.0000,  ..., 0.4556, 0.0736, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[1.3648, 1.2430, 0.7147,  ..., 1.2850, 1.3120, 0.8417],\n          [1.2419, 1.2523, 0.9012,  ..., 0.9149, 1.2638, 0.9624],\n          [0.8477, 1.4177, 1.0887,  ..., 1.3017, 0.8285, 1.2788],\n          ...,\n          [0.8150, 1.3135, 1.0638,  ..., 1.0162, 1.0456, 0.9652],\n          [1.2507, 1.1597, 1.2638,  ..., 0.6510, 1.0451, 1.1983],\n          [0.7626, 0.9073, 0.7679,  ..., 1.2850, 0.9327, 0.8350]],\n\n         [[1.3113, 1.1873, 0.6493,  ..., 1.2301, 1.2575, 0.7786],\n          [1.1862, 1.1968, 0.8391,  ..., 0.8531, 1.2084, 0.9015],\n          [0.7847, 1.3651, 1.0301,  ..., 1.2471, 0.7652, 1.2237],\n          ...,\n          [0.7513, 1.2590, 1.0048,  ..., 0.9563, 0.9863, 0.9044],\n          [1.1951, 1.1024, 1.2085,  ..., 0.5843, 0.9857, 1.1417],\n          [0.6980, 0.8454, 0.7034,  ..., 1.2300, 0.8713, 0.7717]],\n\n         [[0.9628, 0.8753, 0.4956,  ..., 0.9054, 0.9248, 0.5869],\n          [0.8745, 0.8819, 0.6296,  ..., 0.6395, 0.8902, 0.6736],\n          [0.5912, 1.0007, 0.7643,  ..., 0.9174, 0.5774, 0.9010],\n          ...,\n          [0.5676, 0.9259, 0.7465,  ..., 0.7122, 0.7334, 0.6756],\n          [0.8807, 0.8154, 0.8902,  ..., 0.4498, 0.7330, 0.8431],\n          [0.5300, 0.6340, 0.5338,  ..., 0.9054, 0.6523, 0.5820]]]],\n       grad_fn=<ReluBackward0>)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(b1_1(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1,96,96)\n",
    "model = Inception(256, (160, 320), (32, 128), 128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[1.1698, 1.4896, 1.2388,  ..., 1.5423, 1.5226, 1.3114],\n          [1.3956, 0.8639, 1.6647,  ..., 1.2657, 1.2344, 1.0499],\n          [1.5574, 0.9395, 1.1629,  ..., 1.0868, 1.2446, 1.4519],\n          ...,\n          [1.0377, 1.1413, 1.6226,  ..., 1.1727, 0.9363, 1.0025],\n          [1.3389, 1.1335, 1.3113,  ..., 1.3994, 1.4748, 1.5740],\n          [0.8460, 1.4038, 1.3191,  ..., 1.4719, 0.8545, 0.9190]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.1978, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.1150, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.1185, 0.0460],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2173, 0.0000, 0.0000,  ..., 0.0000, 0.2080, 0.1374]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[0.4462, 0.5050, 0.5050,  ..., 0.5155, 0.4639, 0.4573],\n          [0.4690, 0.5050, 0.5050,  ..., 0.5155, 0.4639, 0.4573],\n          [0.4690, 0.5050, 0.5050,  ..., 0.4881, 0.5000, 0.5000],\n          ...,\n          [0.3955, 0.5140, 0.5140,  ..., 0.4798, 0.4798, 0.4798],\n          [0.4174, 0.4909, 0.4909,  ..., 0.4412, 0.4746, 0.4746],\n          [0.4174, 0.4174, 0.4174,  ..., 0.4412, 0.4746, 0.4746]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n       grad_fn=<CatBackward0>)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class Inception2(nn.Module):\n",
    "    # `c1`--`c4` are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception2, self).__init__(**kwargs)\n",
    "        # Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\huan_shit\\Study_Shit\\Deep_Learning\\Dive_into_Deep_Learning\\venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.4401, 0.4401, 0.4845,  ..., 0.4354, 0.4610, 0.4610],\n          [0.4193, 0.4193, 0.4193,  ..., 0.4354, 0.4610, 0.4610],\n          [0.4193, 0.4193, 0.4193,  ..., 0.4650, 0.4650, 0.4741],\n          ...,\n          [0.4206, 0.4206, 0.4604,  ..., 0.4440, 0.4228, 0.4228],\n          [0.4604, 0.4604, 0.4146,  ..., 0.4394, 0.4717, 0.5059],\n          [0.4852, 0.4800, 0.4146,  ..., 0.4394, 0.4717, 0.5059]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n       grad_fn=<CatBackward0>)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Inception2(256, (160, 320), (32, 128), 128)\n",
    "model2(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET1(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (re): ReLU()\n",
      ")\n",
      "NET2(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NET1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET1, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.re = nn.ReLU()  # Module activation function\n",
    "\n",
    "    def foreward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(x)\n",
    "        out = self.re()\n",
    "        return out\n",
    "\n",
    "\n",
    "class NET2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET2, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "\n",
    "    def foreward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(x)\n",
    "        out = F.relu(out)  # function activation function\n",
    "        return out\n",
    "\n",
    "\n",
    "net1 = NET1()\n",
    "net2 = NET2()\n",
    "print(net1)\n",
    "print(net2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    \"\"\"We need to go deeper\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1. Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # 2. Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # 2. Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # 2. Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        bn1 = F.relu(self.b1_1(X))\n",
    "        bn2 = F.relu(self.b2_1(F.relu(self.b2_2(X))))\n",
    "        bn3 = F.relu(self.b3_1(F.relu(self.b3_2(X))))\n",
    "        bn4 = F.relu(self.b4_1(F.relu(self.b4_2(X))))\n",
    "        return torch.cat((bn1, bn2, bn3, bn4), dim=1)\n",
    "\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.net = nn.Sequential(self.block_1(),\n",
    "                                 self.block_2(),\n",
    "                                 self.block_3(),\n",
    "                                 self.block_4())\n",
    "\n",
    "    def block_1(self):\n",
    "        \"\"\"\n",
    "        7x7 Conv2d ->> 3x3 MaxPool2d -> 1x1 Conv2d -> 3x3 Conv2d -> 3x3 MaxPool2d\n",
    "        \"\"\"\n",
    "        b1 = nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(92, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        return b1\n",
    "\n",
    "    def block_2(self):\n",
    "        \"\"\"\n",
    "        2 x Inception Block -> MaxPool2d\n",
    "        \"\"\"\n",
    "        b2 = nn.Sequential(\n",
    "            Inception(64, (96, 128), (16, 32), 32),\n",
    "            Inception(128, (128, 192), (32, 96), 64),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        return b2\n",
    "\n",
    "\n",
    "    def block_3(self):\n",
    "        \"\"\"\n",
    "        5 x Inception Block -> MaxPool2d\n",
    "        \"\"\"\n",
    "        b3 = nn.Sequential(\n",
    "            Inception(192, (96, 208), (16, 48), 64),\n",
    "            Inception(160, (112, 224), (24, 64), 64),\n",
    "            Inception(128, (128, 256), (24, 64), 64),\n",
    "            Inception(112, (144, 288), (32, 64), 64),\n",
    "            Inception(256, (160, 320), (32, 128), 128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        return b3\n",
    "\n",
    "    def block_4(self):\n",
    "        \"\"\"\n",
    "        2 x Inception Block -> AvgPool -> FC\n",
    "        \"\"\"\n",
    "        b4 = nn.Sequential(\n",
    "            Inception(256, (160, 320), (32, 128), 128),\n",
    "            Inception(384, (192, 384), (48, 128), 128),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(10),\n",
    "        )\n",
    "        return b4\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.net(X)\n",
    "        return X\n",
    "\n",
    "    def layer_summary(self, X_shape):\n",
    "        X = torch.rand((1, 1, 96, 96))\n",
    "        for layer in self.net:\n",
    "            X = layer(X)\n",
    "            print(layer.__class__.__name__, \"Output shape:\", X.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "model = GoogLeNet()\n",
    "X = torch.rand(1, 1,96, 96)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0404,  0.0594,  0.0210,  0.0421,  0.0283, -0.0346,  0.0157,  0.0027,\n          0.0171,  0.0023]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Output shape: torch.Size([1, 92, 12, 12])\n",
      "Sequential Output shape: torch.Size([1, 352, 6, 6])\n",
      "Sequential Output shape: torch.Size([1, 576, 3, 3])\n",
      "Sequential Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model.layer_summary((1, 1,96, 96))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}